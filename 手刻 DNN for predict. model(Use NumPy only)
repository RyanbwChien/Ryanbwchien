
import numpy as np
from random import seed
from random import random
from math import exp

# Initialize a network
def initialize_network(n_inputs, n_hidden):
    network = list()
    hidden_layer1 = np.array([random() for i in range((n_inputs + 1)*n_hidden)]).reshape(n_hidden,(n_inputs + 1))
    network.append(hidden_layer1)
    return network
# Initialize a network
def initialize_network_output(n_hidden, n_outputs):
    network = list()
    output_layer = np.array([random() for i in range((n_hidden + 1)*n_outputs)]).reshape(n_outputs,(n_hidden + 1))
    network.append(output_layer)
    return network

# Transfer neuron sigmoid activation
def sigmoid(activation):
	return np.array([1.0 / (1.0 + exp(-activation[i])) for i in range(len(activation))])
# Transfer neuron linear activation
def linear(activation):
	return np.array([activation[i] for i in range(len(activation))])

# Forward propagate input to a network output
# 一次對一個個體的X P個變量
def forward_propagate(model, x):
    inputs = np.append(1,x) 
    inneroutput=[]
    count=1
    for layer in model:
        summation = layer.dot(np.array(inputs))
        if(count!=len(model)):
            a=sigmoid(summation)
            count+=1
        else:
            a=linear(summation)

        inneroutput.append(a)
        inputs = np.append(1,a)
    return [inputs[1:],inneroutput]

# Loss function derivative
def lossfunction(y,yhat):
    sse=0
    for i in range(len(yhat)):
        sse+=(yhat[i] - y[i])**2
    mse=1/len(y)*sse
    return  (mse)
def lossfunction_derivative(y,yhat):
	return  np.array([yhat[i] - y[i] for i in range(len(yhat))] )

# Activity function derivative
def transfer_derivative(output):
	return np.array([output[i] * (1.0 - output[i]) for i in range(len(output))])
def linear_derivative(output):
	return np.array([1 for i in range(len(output))])

def corresmulti(x,y):
    return np.array([x[i]*y[i] for i in range(len(x))]).reshape(-1,1)


i=1
# Backpropagate 
def backward_propagate_error(model, y, yhat,x):
    dL_div_dyhat=lossfunction_derivative(y,yhat[0])
    dyhat_div_dzo=linear_derivative(yhat[0])
    end=dL_div_dyhat*dyhat_div_dzo
    end=np.array(end)
    dL_div_weight = list()   
    temp=[]
    for i in reversed(range(len(model))):
		#layer = network[i]
        temp=[]
        if i==len(model)-1:
            for j in end.reshape(-1,1):
                temp.append(np.append(1,yhat[1][i-1])*j) # hidden layer output layer weight (K+1)*O
            dL_div_weight.append(np.array(temp))
 
            dz_div_da=model[i].T[1:,:].dot(end.reshape(-1,1)) # Output and 'K' hidden layer (K+1)weight backward output layer weight*end
            da_div_dz=transfer_derivative(yhat[1][i-1]) # K-th hidden layer a into transfer_derivative
            end=corresmulti(dz_div_da , da_div_dz)
        if (i!=0 and i!=len(model)-1):
            for j in end.reshape(-1,1):
                temp.append(np.append(1,yhat[1][i-1])*j) # hidden layer output layer weight (K+1)*O
            dL_div_weight.append(np.array(temp))
            dz_div_da=model[i].T[1:,:].dot(end.reshape(-1,1)) # Output and K hidden layer (K+1)weight backward output layer weight*end
            da_div_dz=transfer_derivative(yhat[1][i-1]) # K-th hidden layer a into transfer_derivative
            end=corresmulti(dz_div_da , da_div_dz)    
        if i==0:
            for jj in end.reshape(-1,1):
                temp.append(np.append(1,x)*jj)
            dL_div_weight.append(np.array(temp))
    return(dL_div_weight)

# gradient descent method 
# batch_size=1 means SGD
# batch_size>1 <n means Mini batch GD   
# batch_size=n means GD   
    
batch_size=10 
def MBGD(x,model,y,batch_size):
    total=[]
    idx=np.random.choice(len(x), size=batch_size, replace=False)
    count=0
    #idx=np.random.randint(0,len(x), size=batch_size)
    for j in range(len(model)):  
        for i in idx:
            yhat=forward_propagate(model, x[i,:])
            dL_div_weight=backward_propagate_error(model, y[i], yhat,x[i,:])

            if count==0:
                a=np.array(dL_div_weight[j])
            else:
                a=np.array(dL_div_weight[j])+a
            count+=1
        count=0
        total.append(1/len(x)*a)
    return (total)

def train_network_MBGD(model, train, l_rate, n_epoch,y,batch_size):
    for epoch in range(n_epoch):
        for j in range(len(model)):  
            model[j]+= -(l_rate * MBGD(x,model,y,batch_size)[len(model)-1-j])
    #print('>epoch=%d, lrate=%.3f, error=%.3f' % (epoch, l_rate, sum_error))
    return(model)

# Make a prediction with a network
def predict(anetwork, x):
	outputs = [forward_propagate(anetwork, x[i,:])[0] for i in range(len(x))]
	return outputs 

# Generate dataset
seed(1)
mu=[5,6,7,8,9]
sigma =[5,4,3,2,1]

def genernormal(mu,sigma,n):
    for i in range(len(mu)):
        if i==0:
            data=np.random.normal(mu[i], sigma[i],size=(n,1))      
        else:
            data=np.concatenate(  (data,np.random.normal(mu[i], sigma[i],size=(n,1))),axis=1 )
    return(data)
i=1
def normallization(x):
    for i in range(np.size(x,1)):
        for j in range(np.size(x,0)):
            x[j,i]=(x[j,i]-min(x[:,i]))/(max(x[:,i])-min(x[:,i]))
    return(x)



x= genernormal(mu,sigma,n=40)
x=normallization(x)
y=np.random.normal(100, 10, size=(40,1))  


# User defined #input /hidden / output layer count   
model=[]
model.append(initialize_network(5, 6)[0])
model.append(initialize_network(6, 5)[0])
#model.append(initialize_network(10, 10)[0])
#model.append(initialize_network(10, 10)[0])
#model.append(initialize_network(10, 10)[0])
model.append(initialize_network_output(5, 1)[0])

yyhat=np.array(predict(model, x))
sum_error=lossfunction(y,yyhat)

anetwork=train_network_MBGD(model, x, 0.1, 50000 ,y,4)  # 分10個BATCH N=40故每一BATCH是4 
yyhat=np.array(predict(anetwork, x))
sum_error=lossfunction(y,yyhat)
